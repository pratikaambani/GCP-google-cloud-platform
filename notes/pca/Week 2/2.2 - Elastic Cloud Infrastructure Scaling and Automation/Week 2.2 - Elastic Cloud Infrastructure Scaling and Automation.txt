X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X
X--- Elastic Cloud Infrastructure: Scaling and Automation --X
X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X--X

* 2 Week course: 29 September - 3 October - 5 October
* Course URL: https://www.coursera.org/learn/gcp-infrastructure-scaling-automation/home
* Previous: https://www.coursera.org/learn/gcp-infrastructure-core-services/home/welcome
* Next: https://www.coursera.org/learn/gcp-infrastructure-containers-services

#######
#Week1#
#######

--------------------------
| Module 0: Introduction |
--------------------------
In this module we introduce the Architecting with Google Compute Engine specialization. This specialization is defined for cloud solution architects, DevOps engineers, and anyone who's interested in using GCP, to create new solutions or to integrate existing systems, application environments, and infrastructure with a focus on Compute Engine.

0.1 Welcome to Elastic Cloud Infrastructure: Scaling and Automation
-------------------------------------------------------------------

0.1.1 Video: Course Introduction
0.1.2 Reading: Welcome to Elastic Cloud Infrastructure: Scaling and Automation
0.1.3 Reading: How to download course resources
-----
Interconnecting Networks
Load Balancing and Autoscaling
Infrastructure Automation
Managed Services


--------------------------------------
| Module 1: Interconnecting Networks |
--------------------------------------
In this module, we’ll focus on GCP’s hybrid connectivity products, which are Cloud VPN, Cloud Interconnect, and Peering. We’ll also look at options for sharing VPC networks within GCP.

* Objectives
	Recall the GCP interconnect and peering services available to connect your infrastructure to GCP
	Determine which GCP interconnect or peering service to use in specific circumstances
	Create and configure VPN gateways
	Recall when to use Shared VPC and when to use VPC Network Peering

1.1 Module Overview
-------------------

1.2 Cloud VPN
-------------

1.2.1 Video: Cloud VPN
-----
Securely connects your on-premices network to GCP VPC network
SLA: 99.9%

Cloud VPN securely connects your on-premise network to your GCP VPC network through an IPSec VPN tunnel. 
Traffic traveling between the two networks is encrypted by one VPN gateway. Then decrypted by the other VPN gateway. This protects your data as it travels over the public internet. That's why Cloud VPN is useful for low volume data connections. 
Supports:
	site to site VPN 
	static routes
	dynamic routes
	IKEv1 and IKEv2 ciphers

Cloud VPN doesn't support new cases where a client computers need to dial in to a VPN using client VPN software. 

Example of Cloud VPN:
	Check Image: ECISA-1.2.1.1.jpg
	This diagram shows a simple VPN connection between your VPC and on-premise network. 
	Your VPC network has subnets in US-east one and US-west one, with GCP resources in each of those regions. 
	These resources are able to communicate using their internal IP addresses because routing within a network is automatically configured, assuming that firewall rules allow the communication. 
	Now, in order to connect to your on-premise network and its resources you need to configure your Cloud VPN gateway on-premise VPN gateway and to VPN tunnels. 
	The Cloud VPN gateway is a regional resource that uses a regional external IP address. 
	Your on-premise VPN gateway can be a physical device in your data center or a physical or software based VPN offering in another Cloud providers network. 
	This VPN gateway also has an external IP address. 
	A VPN tunnel then connects your VPN gateways and serves as the virtual medium through which encrypted traffic is passed. 
	In order to create a connection between two VPN gateways you must establish two VPN tunnels. 
	Each tunnel defines the connection from the perspective of its gateway and traffic can only pass when the pair of tunnels established. 
	Now, one thing to remember when using Cloud VPN is that the maximum transmission unit or MTU for your on-premises VPN gateway cannot be greater than 1,460 bytes. This is because of the encryption and encapsulation of packets. 

Example of Cloud VPN:
	Check Image: ECISA-1.2.1.2.jpg
	In order to use dynamic routes you need to configure Cloud Router. 
	Cloud Router can manage routes from Cloud VPN tunnel using border gateway protocol or BGP. 
	This routing method allows for routes to be updated and exchanged without changing the tunnel configuration. 
	For example, this diagram shows two different regional subnets in a VPC network namely tests and prod. 
	The on-premise network has 29 subnets and the two networks are connected through Cloud VPN tunnels. 
	Now, how would you handle adding new subnets? For example, how would you add a new staging subnet in the GCP network and a new on-premise 10.0.30.0/24 subnet to handle growing traffic in your data center? To automatically propagate network configuration changes the VPN tunnel uses Cloud Router to establish a BGP session between the VPC and the on-premise VPN gateway which must support BGP. The new subnets are then seamlessly advertised between networks. This means that instances in the new subnets can start sending and receiving traffic immediately as you will explore in the upcoming lab. To set up BGP an additional IP address has to be assigned to each end of the VPN tunnel. These two IP addresses must be link-local IP addresses. Belonging to the IP address range 169.254.0.0/16. These addresses are not part of IP address space of either network and are used exclusively for establishing a BGP session.
	
1.2.2 Video: Lab Intro: Virtual Private Networks (VPN)
1.2.3 Video: Getting Started with Google Cloud Platform and Qwiklabs
-----
Setting up VPN tunnels
between two networks
in separate regions

So that: A VM in one network can ping a VM in the other network over its internal IP address.

1.2.4 Graded External Tool: Graded External ToolLab: Virtual Private Networks (VPN)
1.2.5 Video: Lab Review: Virtual Private Networks (VPN)
-----
Objectives
	Create VPN gateways in each network
	Create VPN tunnels between the gateways
	Verify VPN connectivity

Check lab: 1.2.5 Virtual Private Networks (VPN) _ Qwiklabs.zip

1.3 Cloud Interconnect and Peering
----------------------------------

1.3.1 Video: Cloud Interconnect and Peering
-----
There are different Cloud Interconnect and Peering services available to connect your infrastructure to Google's network. 

Check Image: ECISA-1.3.1.1.jpg
These services can be split into dedicated versus shared connections and layer two verses layer three connections. 
The services are Direct Peering, Carrier Peering, Dedicated Interconnect, and Partner Interconnect. 

Dedicated connections provide a direct connection to Google's network. 
Shared connections provide a connection to Google's network through a partner. 

Layer two connections use a VLAN that pipes directly into your GCP environment, providing connectivity to internal IP addresses in the RFC 1918 address space. 
Layer three connections provide access to G Suite services, YouTube and Google Cloud APIs using public IP addresses. 

Cloud VPN: 
	Uses the public Internet but traffic is encrypted and provides access to internal IP addresses. That's why Cloud VPN is a useful addition to Direct Peering and Carrier Peering. 

1.3.2 Video: Cloud Interconnect
-----

Check Image: ECISA-1.3.2.1.jpg
Dedicated Interconnect:
	Dedicated Interconnect provides direct physical connections between your On-premise network and Google's network. This enables you to transfer a large amount of data between networks which can be more cost-effective than purchasing additional bandwidth over the public Internet. 
	In order to use Dedicated Interconnect, you need to provision a cross-connect between the Google network and your own router in a common co-location facility, as shown in this diagram. 
	To exchange routes between the networks, you configure a BGP session over the interconnect between the Cloud Router and the On-premise router. 
	This will allow user traffic from the on-premise network to reach GCP resources on the VPC network and vice-versa. 
	SLA: 99.9 percent or a 99.99 percent uptime. 
	In order to use Dedicated Interconnect, your network must physically meet Google's network in a supported co-location facility. 

Check Image: ECISA-1.3.2.2.jpg
Partner Interconnect:
	What if I am nowhere near one of these locations?
	That's when you want to consider a Partner Interconnect. 
	Partner Interconnect provides connectivity between your on-premise network and your VPC network through a supported service provider. 
	This is useful if your data center is in the physical location that cannot reach a Dedicated Interconnect co-location facility or if your data needs don't warrant a Dedicated Interconnect. 
	In order to use Partner Interconnect, you work with the supported service provider to connect your VPC and on-premise networks. 
	For a full list of providers, see the link section of this video. 
	These service providers have existing physical connections to Google's network that they make available for their customers to use. 
	After you establish connectivity with the service provider, you can request a Partner Interconnect connection from your service provider then establish a BGP session between your Cloud Router and On-premise Router to start passing traffic between your networks via the service providers network. 
	SLA: 99.9 percent or 99.99 percent uptime SLA between Google and the service provider. 
	See the Partner Interconnect documentation for details on how to achieve these SLAs. 

Comparing Interconnect Options:	
	Check Image: ECISA-1.3.2.3.jpg

1.3.3 Video: Peering
-----
Peering:
	These services are useful when you require access to Google and Google Cloud properties. 
	Google allows you to establish a direct peering connection between your business network and Google's. 
	With this connection, you will be able to exchange internet traffic between your network and Google's at one of the Google's broad-reaching Edge network locations. 
	Direct Peering and Carrier Peering

Direct Peering:
	Direct Peering with Google is done by exchanging BGP routes between Google and the peering entity. 
	After a direct peering connection is in place, you can use it to reach all the Google's services including the full suite of Google Cloud Platform products. 
	Unlike dedicated interconnect, direct peering does not have an SLA. 
	In order to use direct peering, you need to satisfy the peering requirements in the links section of this video. 
	GCP's Edge points of presence or PoPs are where Google's network connects to the rest of the internet via peering. 
	PoPs are present on over 90 internet exchanges and at over 100 interconnection facilities around the world. 
	For more information about these exchange points and facilities, I recommend looking at Google's Peering DB entries, which are linked below this video. 

Carrier Peering:
	If you look at this map and say, "hey, I am nowhere near one of these locations," you will want to consider Carrier Peering. 
	If you require access to Google public infrastructure and cannot satisfy Google's peering requirements, you can connect via a carrier peering partner. 
	Work directly with your service provider to get the connection you need and to understand the partners requirements. 
	For a full list of available service providers, see the links section of this video. 
	Now, just like direct peering, carrier peering also does not have an SLA. 

Comparing Peering options:
	Check Image: ECISA-1.3.3.1.jpg
	All of these options provide public IP address access to all of Google's services. 
	The main differences are capacity and the requirements for using a service. 
	Direct peering has a capacity of 10Gbps per link and requires you to have a connection in a GCP Edge point of presence. 
	Carrier peering's capacity and requirements vary depending on the service provider that you work with.

1.3.4 Video: Choosing a connection
-----
Check Image: ECISA-1.3.4.1.jpg

5 ways

dedicated versus shared connections 
Layer 2 versus Layer 3 connections

Another way:
Check Image: ECISA-1.3.4.2.jpg
	by interconnect services and by peering services
	Interconnect services provide direct access to RFC1918 IP addresses in your VPC with an SLA. 
	Peering services in contrast offer access to Google public IP addresses only without an SLA. 
	
Ask yourself whether you need to extend your network for G Suite services, YouTube or Google Cloud APIs. If you do, choose one of the peering services. If you can meet Google's direct peering requirements, choose Direct Peering. Otherwise, choose Carrier Peering. If you don't need to extend your network for G Suite services or Google Cloud APIs, but want to extend the reach of your network to GCP, you want to pick one of the interconnect services. If you cannot meet Google at one of its co-location facilities, choose Cloud VPN or Partner Interconnect. This choice will depend on your bandwidth and encryption requirements along with the purpose of the connection. Specifically, if you have modest bandwidth needs, will use the connection for short durations and trials, and require an encrypted channel, choose Cloud VPN. Otherwise, choose Partner Interconnect. If you can meet Google at one of its co-location facilities you, might jump to Dedicated Interconnect. However, if you cannot provide your own encryption mechanism for sensitive traffic, feel that a 10 Gbps connections is too big or won't access to multiple clouds, you will want to consider Cloud VPN or Partner Interconnect instead.

1.4 Shared VPC and VPC Peering
------------------------------

In the simplest Cloud environment, a single project might have one VPC network, spanning many regions with VM instances hosting very large and complicated applications. 
However, many organizations commonly deploy multiple isolated projects with multiple VPC networks and sublets. 

In this lesson, we are going to cover two configurations for sharing VPC networks across GCP projects. 

First: 	We will go over shared VPC which allows you to share a network across several projects in your GCP organization. 
Second: We will go over VPC Network Peering which allows you to configure private communication across projects in same or different organizations.

1. Shared VPC
Check Image: ECISA-1.4.1.1.jpg
Host Project: One SPOC, exposed over internet
Service Projects: rest of servers are internal and not exposed over internet

2. VPC Peering
Check Image: ECISA-1.4.1.2.jpg
Decentralized or distributed approach to multiproject networking: each VPC network, may remain under the control of separate administrator groups, and maintains its own global firewall, and routing tables. 

In order for VPC Network Peering to be established successfully, the producer network admin needs to peer the producer network with the consumer network. The consumer network admin needs to peer the consumer network with the producer network. When both peering connections are created, the VPC Network Peering session becomes active and routes are exchanged. 
This allows the virtual machine instances to communicate privately using their internal IP addresses. 


Comparison:
	Check Image: ECISA-1.4.1.3.jpg
	If you want to configure a private communication between VPC networks in different organizations, you have to use VPC Network Peering.
	Shared VPC only works within the same organization. 
	Somewhat similarly, if you want to configure private communication between VPC networks in the same project, you have to use VPC Network Peering
	Shared. VPC, only works across projects. 
	
	Biggest difference : network administration models. 
		Shared VPC is a centralized approach to multi-project networking because security and network policy occurs in a single designated VPC network. In contrast, VPC Network Peering is a decentralized approach because each VPC network can remain under the control of separate administrator groups, and maintains its own global firewall, and routing tables. If you want to learn more about shared VPC and VPC Peering, I recommend the networking in Google Cloud Platform course, which you can find in the links section of this video.

1.5 Review
----------

1.5.1 Quiz: Module Quiz
-----
Done - 4 questions

1.5.2 Video: Module Review
-----

five different ways of connecting your infrastructure to GCP :
	Dedicated Interconnect
	Partner Interconnect
	Cloud VPN
	Direct Peering
	Carrier Peering

--------------------------------------------
| Module 2: Load Balancing and Autoscaling |
--------------------------------------------
In this module, we will cover the different types of load balancers that are available in GCP. We will also go over managed instance groups and their autoscaling configurations, which can be used by these load balancing configurations.

* Objectives
	Recall the various load balancing services
	Determine which GCP load balancer to use in specific circumstances
	Describe autoscaling behavior, policies, configuration, and metrics
	Configure load balancers and autoscaling

2.1 Module Overview
------------
Fully managed 
>1mn queries/sec
fully distributed software defined managed service
it is not instance or device based

Types:
	Global: when users are globally distributed
		HTTP/S
		SSL Proxy
		TCP Proxy
	Regional: internal
		Internal TCP/UDP Andromeda
		Network TCP/UDP Maglev
		Internal HTTP/S

2.2 Managed instance groups
---------------------------

2.2.1 Video: Managed instance groups
-----
MIG: 
	collection of identical virtual machine instances that you control as a single entity using an instance template. 
	You can easily update all the instances in the group by specifying a new template in a rolling update. 
	Also, when your application requires additional compute resources, managed instance groups can automatically scale the number of instances in the group. 
	Managed instance groups can work with load balancing services to distribute network traffic to old instances in the group. 
	If an instance in the groups stops, crashes, or is deleted by an action other than the instance groups command, the managed instance group automatically recreates the instance so it can resume its processing tasks. 
	The recreated instance uses the same name and the same instance template as the previous instance. 
	Managed instance groups can automatically identify and recreate unhealthy instances in a group to ensure that all the instances are running optimally. 
	
	Regional managed instance groups are generally recommended over zonal managed instance groups because they allow you to spread the application load across multiple zones instead of confining your application to a single zone or you're having to manage multiple instance groups across different zones. 
	This replication protects against zonal failures and unforeseen scenarios where an entire group of instances in a single zone malfunctions. 
	If that happens, your application can continue serving traffic from instances running in another zone of the same region. 
	
	In order to create a managed instance group, you first need to create an instance template. 
	Next, you are going to create a managed instance group of end specific instances. 
	The instance group manager then automatically populates the instance group based on the instance template. 
	You can easily create instance templates using the GCP console. 
	The instance template dialogue looks and works exactly like creating an instance, except that the choices are recorded so that they can be repeated. 
	When you create an instance group, you define the specific rules for the instance group. 
		First, decide whether the instance group is going to be single or multi zoned and where those locations will be. 
		Second, choose the ports that you are going to allow and load balance across. 
		Third, select the instance template that you want to use. 
		Fourth, decide whether you want to auto-scale and under what circumstances. 
		Finally, consider creating a health check to determine which instances are healthy and should receive traffic. 
		Essentially, you're still creating virtual machines, but you're applying more rules to that instance group.

2.2.2 Video: Autoscaling and health checks of MIGs
-----
You just define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load. 

Dynamically add/remove instances:
	Upscale - when load increases
	Downscale - when load decreases

Autoscaling policy:
	CPU Utilization
	LB capacity
	monitoring metrics
	Queue-based workload	

Monitoring:
	Now, you might ask yourself, how do I monitor the utilization of my instance group? When you click on an instance group or even an individual virtual machine, a graph is presented. By default, you will see the CPU utilization over the past hour. But you can change the timeframe and visualize other metrics like disk and network usage. These graphs are very useful for monitoring your instances, utilization, and for determining how best to configure your autoscaling policy to meet changing demands. 
	If you monitor the utilization of your virtual machine instances and Stackdriver monitoring, you can even set up alerts through several notification channels. For more information on autoscaling, see the links section of this video. 

Health Check:
	Another important configuration for a managed instance group and load balancer is a health check. 
	A health check is very similar to an Uptime check in Stackdriver. 
	You just define a protocol, port, and health criteria as shown in the screenshot. 
	Based on this configuration, GCP computes a health state for each instance. 
	The health criteria defines how often to check whether an instance is healthy. That's the check interval. 
	How long to wait for a response? That's the timeout. 
	How many successful attempts are decisive? That's the healthy threshold. 
	How many failed attempts are decisive? That the unhealthy threshold. 

2.3 HTTP(S) load balancing
--------------------------

2.3.1 Video: Overview of HTTP(S) load balancing
-----
Acts at layer seven of the OSI model. 
This is the application layer which deals with the actual content of each message allowing for routing decisions based on the URL. 

GCP HTTPS load balancing provides:
	Global load balancing for HTTPS requests destined for your instances. 
	This means that your applications are available to your customers at a single any cast IP address, which simplifies your DNS setup. 
	HTTPS load balancing balances HTTP and HTTPS traffic across multiple backend instances and across multiple regions. 
	HTTP requests are load balanced on port 80 or 8080, and HTTPS requests are load balanced on port 443. 
	This load balancers supports both IPv4 and IPv6 clients, is scalable, requires no pre-warming, and enables content-based and cross-regional load balancing. 
	You can configure url maps that route some URLs to one set of instances and route other URLs to other instances. 

Requests are generally routed to the instance group that is closest to the user. If the closest instance group does not have sufficient capacity, the request is sent to the next closest instance group that does have the capacity. 

Architecture:
	Check Image: ECISA-1.4.1.3.jpg
	A Global Forwarding Rule direct incoming requests from the Internet to a target HTTP proxy. 
	The target HTTP proxy checks each request against a URL map to determine the appropriate backend service for the request. 
		For example, you can send requests for www.example.com slash audio to one backend service, which contains instances configured to deliver audio files, and the request for www.example.com/video to another backend service which contains instances configured to deliver video files. 
	The backend service directs each request to an appropriate backend based on solving capacity zone and instance held of its attached backends. 
	
	Backend Services:
	The backend services contain 
		health check
		session affinity
		a timeout setting
		and one or more backends. 
	A health check pulls instances attached to the backend service at configured intervals. 
	Instances that pass the health check are allowed to receive new requests. 
	Unhealthy instances are not sent requests until they are healthy again. 
	Normally, HTTPS load balancing uses a round robin algorithm to distribute requests among available instances. 
	This can be overridden with session affinity. Session affinity attempts to send all requests from the same client to the same Virtual Machine Instance. 
	Backend services also have a timeout setting, which is set to 30 seconds by default. 
	This is the amount of time the backend service will wait on the backend before considering the request a failure. 
	This is a fixed timeout not an idle timeout. If you require longer lived connections, set this value appropriately. 
	The backends themselves contain an instance group, a balancing mode, and a capacity scalar. 
	An instance group contains Virtual Machine Instances. 
	The instance group may be a managed instance group with or without autoscaling or an unmanaged instance group. 
	A balancing mode tells the load balancing system how to determine when the backend is at full usage. 
	If older backends for the backend service in a region are at the full usage, new requests are automatically routed to the nearest region that can still handle requests. 
	The balancing mode can be based on CPU utilization or requests per second. 
	A capacity setting is an additional control that interacts with the balancing mode setting. 
		For example, if you normally want your instances to operate at a maximum of 80 percent CPU utilization, you would set your balancing mode to 80 percent CPU utilization and your capacity to 100 percent. 
		If you want to cut instance utilization in half, you could leave the balancing mode at 80 percent CPU utilization and set capacity to 50 percent. Now, any changes to your backend services are not instantaneous. So don't be surprised if it takes several minutes for your changes to propagate throughout the network.

2.3.2 Video: Example: HTTP load balancer
-----

Check Image: ECISA-2.3.2.1.jpg
Cross region LB
Content based LB: Image and Video handled by separate servers


2.3.3 Video: HTTP(S) load balancing
-----

An HTTP(S) load balancer has the same basic structure as the HTTP load balancer, but differs in the following ways: 
	An HTTP(s) load balancer uses a target HTTPS proxy instead of a target HTTP proxy. 
	An HTTPS load balancer requires at least one signed SSL certificate installed on the target HTTPS proxy for the load balancer. 
	The client SSL session terminates at the load balancer. 
	HTTPS load balancer support the QUIC transport layer protocol.
	QUIC is a transport layer protocol that allows faster client connection initiation, eliminates head of line blocking in multiplexed streams, and supports connection migration when a client's IP address changes. 

SSL Certificates:
	To use HTTPS, you must create at least one SSL certificate that can be used by the target proxy for the load balancer. 
	You can configure the target proxy with up to 10 SSL certificates. 
	For each SSL certificate, you first create an SSL certificate resource which contains the SSL certificate information. SSL certificate resources are used only with load balancing proxies such as target HTTPS proxy or target SSL proxy

2.3.4 Video: Lab Intro: Configuring an HTTP Load Balancer with Autoscaling
-----
configure an HTTP load balancer with auto-scaling. 
	create two managed instance groups that serve as backends in US Central one and Europe West One. 
	Then create and stress test a load balancer to demonstrate global load balancing and auto-scaling.

2.3.5 Graded External Tool: Graded External ToolLab: Configuring an HTTP Load Balancer with Autoscaling
2.3.6 Video: Lab Review: Configuring an HTTP Load Balancer with Autoscaling
-----
Objectives
	Create HTTP and health check firewall rules
	Create a custom image for a web server
	Create an instance template based on the custom image
	Create two managed instance groups
	Configure an HTTP load balancer with IPv4 and IPv6
	Stress test an HTTP load balancer

2.4 SSL/TCP proxy load balancing
--------------------------------

2.4.1 Video: SSL proxy load balancing
-----

SSL proxy is a global load balancing service for encrypted, non-HTTP traffic. 
This load balancer terminates user SSL connections at the load balancing layer, 
	then balances the connections across your instances using the SSL or TCP protocols. 
	These instances can be in multiple regions, and the load balancer automatically directs traffic to the closest region that has capacity. 
SSL proxy load balancing supports both IPv4 and IPv6 addresses for client traffic and provides intelligent routing, certificate management, security patching, and SSL policies. 

Intelligent routing: This load balancer can route requests to backend locations where there is capacity. 
Certificate management: 
	you only need to update your customer-facing certificate in one place when you need to switch those certificates.
	you can reduce the management overhead for your virtual machine instances by using self-signed certificates on your instances. 
	In addition, if vulnerabilities arise in the SSL or TCP stack, GCP will apply patches at the load balancer automatically in order to keep your instances safe. 
	

Check Image: ECISA-2.4.1.1.jpg
	This network diagram illustrates SSL proxy load balancing. In this example, traffic from users in Iowa and Boston is terminated at the global load balancing layer. From there, a separate connection established to the closest backend instance. In other words, the user in Boston would reach the US East region, and the user in Iowa would reach the US Central region, if there's enough capacity. Now, the traffic between the proxy and the backend can use SSL or TCP. I recommend using SSL.

2.4.2 Video: TCP proxy load balancing
-----


2.5 Network load balancing
--------------------------

Regional service
non proxied service

All traffic is passed through the load balancer instead of being proxied and traffic can only be balanced between virtual machine instances that are in the same region unlike a global load balancer. 
This LB service uses forwarding rules to balance the load on your systems based on incoming IP protocol data such as address, port, and protocol type. 
You can use it to load balance UDP traffic and to load balance TCP NSSL traffic on ports that are not supported by the TCP proxy and SSL proxy load balancers. 
back ends of a network load balancer can be a template-based instance group or target pooled resource. 

Target pooled resources:
	A group of instances that receive incoming traffic from forwarding rules. 
	When a forwarding rule direct traffic to a target pool, the load balancer picks an instance from these target pools based on hash of the source IP and port, and the destination IP and port. 
		These target pools can only be used with forwarding rules that handled TCP and UDP traffic. 
		Now each project can have up to 50 target pools 
		each target pool can have only one health check. 
		All the instances of a target pool must be in the same region which is the same limitation as for the network load balancer.

2.6 Internal load balancing
---------------------------

2.6.1 Video: Internal load balancing
-----
Regional, Private LB service

this load balancer enables you to run and scale your services behind a private load balancing IP address. This means that it is only accessible through the internal IP address of virtual machine instances that are in the same region. Therefore, use internal load balancing to configure an internal load balancing IP address, to act as the front end to your private backend instances. 
Because you don't need a public IP address for your load balanced service, your internal client requests to stay internal to your VPC network and the region. 
This often results in lower latency, because all your load balanced traffic will stay within Google's network, making your configuration much simpler. 


Benefit of using a software-defined internal load balancing service:
GCP internal load balancing is not based on a device or a virtual machine instance. Instead, it is a software-defined, fully distributed load balancing solution. 


Check Image: ECISA-2.6.1.1.jpg
Proxy internal LBing vs GC Internal LBing:
	In the traditional proxy model of internal load balancing as shown on the left, you configure an internal IP address on a load balancing device or instances, and your client instance connects to this IP address
	Traffic coming to the IP address is terminated at the load balancer, and the load balancer selects a backend to establish a new connection to.
	Essentially, there are two connections. 
		One between the client and the load balancer, 
		and the one between the load balancer and the backend. 
	
	GCP internal load balancing distributes client instance requests to the backend using a different approach, as shown on the right. 
	It uses lightweight load balancing built on top of Andromeda
	Google's network virtualization stack, to provide software-defined load balancing that directly delivers the traffic from the client instance to a backend instance.
	
Internal LBing supports 3-tier web services:
	Check Image: ECISA-2.6.1.2.jpg
	internal load balancing enables you to support use-cases such as the traditional treaty or web service. 
		Example: the web tier uses an external HTTPS load balancer, that provides a single global IP address for users in San Francisco, Iowa and Singapore, and so on. 
	The backends of this load balancer are located in the US-Central1 and Asia-East-1 region, because this is a global load balancer. 
	These backends then access an internal load balancer in each region as the application or internal tier. 
	The backends of this internal tier are located in US-Central1-A, US-Central1-B, and Asia-East1-B. 
	The last tier is the database tier in each of these zones. 
	The benefit of this three-tier approach is that neither the database tier nor the application tier is exposed externally. 
	This simplifies security and network pricing

2.6.2 Video: Lab Intro: Configuring an Internal Load Balancer
-----
Internal load balancer concepts 
	Create two managed instance groups in the same region
	then configure and test the internal load balancer with the instance groups as the backends as shown in this network diagram.
	Check Image: ECISA-2.6.2.1.jpg

2.6.3 Graded External Tool: Graded External ToolLab: Configuring an Internal Load Balancer
2.6.4 Video: Lab Review: Configuring an Internal Load Balancer
-----
Objectives
	Create HTTP and health check firewall rules
	Configure two instance templates
	Create two managed instance groups
	Configure and test an internal load balancer

2.7 Choosing a load balancer
----------------------------

Check Image: ECISA-2.7.1.1.jpg

Support for IPv6 clients:
	One differentiator between the different GCP load balancers is the support for IPv6 clients. 
	Only the HTTPS, SSL proxy, and TCP proxy load balancing services support IPV6 clients. 
	IPv6 termination for these load balancers enables you to handle IPv6 requests from your users and proxy them over IPv4 to your backend. 
		For example, in this diagram, there is a website, www.example.com, that is translated by Cloud DNS to both an IPv4 and IPv6 address. 
		This allows a desktop user in New York and a mobile user in Iowa to access the load balancer through the IPv4 and IPv6 addresses respectively. 
	
	How does the traffic get to the backends and their IPv4 addresses? 
		The load balancer acts as a reverse proxy, terminates the IPv6 client connection and places the request into an IPv4 connection to a backend. 
		On the reverse path, the load balancer receives the IPv4 response from the backend and places it into the IPv6 connection back to the original client. 
	In other words, configuring IPv6 termination for your load balancers, lets your backend instances appear as IPv6 applications to your IPv6 clients. 


Which load balancer best suits your implementation of GCP:
Check Image: ECISA-2.7.1.2.jpg

Global versus regional load balancing, 
external versus internal load balancing, 
traffic type. 

If you need an external load balancing service, start on the top left of this flowchart. First, choose the type of traffic that your load balancer must handle. If that is HTTP or HTTPS traffic, I recommend using the HTTPS load balancing service as a layer seven load balancer. Otherwise, use the TCP and UDP traffic paths of this flowchart to determine whether the SSL proxy, TCP proxy, or network load balancing service meets your needs. If you need an internal load balancing service, you have the internal load balancing service available and it supports both TCP and UDP traffic. As I mentioned at the beginning of this module, there's actually another internal load balancer for HTTPS traffic but it's in beta as of this recording. The sixth load balancer is for HTTP or HTTPS traffic and it's regional meaning for IPv4 clients. 

Check Image: ECISA-2.7.1.3.jpg
This table helps you identify the right load balancer based on the traffic type, the distribution of your backend global or regional, and the type of IP addresses of your backend external or internal. 
This table also lists the available ports for load balancing and highlights that only the Global Load Balancers support both IPv4 and IPV6 clients.

2.8 Review
----------

2.8.1 Quiz: Module Quiz
-----
Done - 3 questions

2.8.2 Video: Module Review
-----
We looked at the different types of load balancers that are available in GCP along with the managed instance groups and auto-scaling. 
We also discussed the criteria for choosing between the different load balancers
Remember, sometimes it's useful to combine an internal and external load balancer to support three tier web services.


#######
#Week2#
#######

---------------------------------------
| Module 3: Infrastructure Automation |
---------------------------------------
In this module, we cover how to use Deployment Manager to automate the deployment of infrastructure and how to use GCP Marketplace to launch infrastructure solutions. You will use Deployment Manager or Terraform to deploy a VPC network, a firewall rule, and VM instances in the lab of this module.

* Objectives
	Automate the deployment of GCP services using Deployment Manager or Terraform
	Outline the GCP Marketplace

3.1 Overview
------------

How to automate the deployment of GCP infrastructure

Calling the Cloud APIs from code is a powerful way to generate infrastructure. But writing code to create infrastructure also has some challenges. 
One issue is that the maintainability of the infrastructure depends directly on the quality of the software. 
For example, a program could have a dozen locations that call the Cloud APIs to create VMs. Fixing a problem with the definition of one VM would require first identifying which of the dozen calls actually created it. Standards software development best practices will apply and it's important to note that things could change rapidly requiring maintenance on your code. Clearly, another level of organization is needed. 

Deployment Manager:
	That's the purpose of deployment manager. Deployment manager uses a system of highly structured templates and configuration files to document the infrastructure in an easily readable and understandable format. 
	Deployment manager conceals the actual Cloud API calls. So you don't need to write code and can focus on the definition of the infrastructure. 
	In this module, we cover how to use deployment manager to automate the deployment of infrastructure, and how to use GCP marketplace to launch infrastructure solutions. 
	You will use Deployment Manager or Terraform to deploy a VPC network, a firewall rule, and virtual machine instances in the lab of this module. 
	I will also demonstrate how to launch infrastructure solutions using GCP marketplace. 
	Let's start by talking about deployment manager.

3.2 Deployment Manager
----------------------

3.2.1 Video: Deployment Manager
-----

New to GCP: Console
Command Line: Cloud Shell
Deployment Manager:
	Deployment Manager takes this one step further. 
	Repeatable deployment process
	Declarative language
	focus on application
	parallel deployment
	template driven
	Benefit:
		It allows you to specify what the configuration should be and let the system figure out the steps to take. 
		Instead of deploying each resource separately, you specify the set of resources which compose the application or service, allowing you to focus on the application. 
		Unlike Cloud Shell, Deployment Manager will deploy resources in parallel. You can even abstract parts of your configuration into individual building blocks or templates that can be used for other configurations. 
	
	Deployment Manager uses the underlying APIs of each GCP service to deploy your resources.
	This enables you to deploy almost everything we have seen so far from instances, instance templates and groups to VPC networks, firewall rules, VPN tunnels, Cloud routers, and load balancers. 
	
Example:
	Auto mode network with HTTP firewall rule
	It's useful to parameterize your configuration with templates. 
	create one template for the auto mode network and one for the firewall rule for reusability of those individual templates
	
	Auto mode:
		written in Jinja2 or Python. 
		each resource must contain a name, type, and properties. 
		name: environment variable to get the name from the top-level configuration which makes this template more flexible
		type: API for a VPC network which is compute.v1.network. 
			  You can find all supported types in the documentation or query them within Cloud Shell
		
		By definition, an auto mode network automatically creates a subnetwork in each region. Therefore, I am setting the auto-create subnetworks property to true. 
		
	HTTP Firewall rule:
		name: environment variable
		type: API for a firewall rule which is compute.v1.firewall. 
		properties: 
			network I want to apply this firewall rule to the source IP ranges, and the protocols, and ports that are allowed. 
			Except for the source IP ranges, I'm defining these properties as template properties. 
			I will provide the exact properties from the top-level configuration, which makes this firewall rule extremely flexible. 
			Essentially, I can use this firewall rule template for any network and any protocol and port combination. 
		
	Next, let's write the top-level configuration in YAML syntax. 
	I start by importing the templates that I want to use in this configuration, which are autonetwork.jinja and firewall.jinja. 
	Then I define the auto mode network by giving it the name mynetwork and leveraging the auto network.jinja template. 
	I could create more auto mode networks in this configuration with other names or simply reuse this template in other configurations later on. 
	
	Now I define the firewall rule by giving it a name, leveraging the firewall.jinja template, referencing my network, and defining the IP protocol and port. 
	I can easily add other ports such as 443 for HTTPS or 22 for SSH traffic. 
	Using the self link reference for the network name ensures that the VPC network is created before the firewalled rule. 
	This is very important because Deployment Manager creates all the resources in parallel unless you use references. 
	You would get an error without the reference because you cannot create a firewall rule for a non-existing network. 
	Now there are other infrastructure automation tools in addition to Deployment Manager that you can use in GCP. 

Other IaaC Tools:
	You can also use Terraform, CHEF, Puppet, Ansible, or Packer. 
	All of these tools allow you to treat your infrastructure like software, which helps you decrease costs, reduce risk, and deploy faster by capturing infrastructure as code. 
	You might recognize some of these tools because they work across many Cloud service providers. I recommend that you provision and manage resources on Google Cloud with the tools you already know. That's why in the upcoming lab, you'll have the choice of using Deployment Manager or Terraform to automate the deployment of Infrastructure.

3.2.2 Video: Lab Intro: Automating the Infrastructure of Networks Using Deployment Manager or Terraform
-----

automate the deployment of VPC networks, firewall rules, and VM instances. 

Two choices: You can deploy resources by using Deployment Manager or Terraform. 
Either way, you deploy an automotive network called my network with a firewall rule to allow HTTP, SSH, RDP, and ICMP traffic. 
You also deploy the VM instances shown in this network diagram. 
Again, you have the choice of deploying these resources with Deployment Manager or Terraform through two separate labs. 
You can complete either lab or even both.

3.2.3 Graded External Tool: Graded External ToolLab: Automating the Infrastructure of networks using Deployment Manager
3.2.4 Video: Lab Review: Automating the Infrastructure of networks using Deployment Manager
-----
Objectives
	Create a configuration for an auto mode network
	Create a configuration for a firewall rule
	Create a template for VM instances
	Create and deploy a configuration
	Verify the deployment of a configuration
	
Check lab: 3.2.3 Automating the Deployment of Infrastructure Using Deployment Manager _ Qwiklabs_files.zip

3.2.5 Graded External Tool: Graded External ToolLab: Automating the Infrastructure of networks using Terraform
3.2.6 Video: Lab Review: Automating the Infrastructure of networks using Terraform
-----
Objectives
	Create a configuration for an auto mode network
	Create a configuration for a firewall rule
	Create a module for VM instances
	Create and deploy a configuration
	Verify the deployment of a configuration

Check lab: 3.2.4 Automating the Deployment of Infrastructure Using Terraform _ Qwiklabs_files.zip

3.3 GCP Marketplace
-------------------

3.3.1 Video: GCP Marketplace
-----
GCP marketplace lets you quickly deploy functional software packages that run on GCP. 
Essentially, GCP marketplace offers production grade solutions from third-party vendors who have already created their own deployment configurations based on Deployment Manager. 
These solutions are built together with all of your projects GCP services. 
If you already have a license for a third party service, you might be able to use a Bring Your Own License solution. 
You can deploy a software package now and scale that deployment later when your applications require additional capacity.
GCP even updates the images of these software packages to fix critical issues and vulnerabilities but doesn't update software that you have already deployed. 
You even get direct access to partner support.

3.3.2 Video: Demo: Launch on GCP Marketplace
-----
Google Cloud Marketplace offers production-grade solutions from third-party vendors who have already created their own deployment configurations based on Deployment Manager.


3.4 Review
----------
3.4.1 Quiz: Module Quiz
-----
Done - 2 questions

3.4.2 Video: Module Review
-----
we automated the deployment of Infrastructure using Deployment Manager and Terraform 

For those who manage several resources and need to deploy, update, and destroy them in a repeatable way Infrastructure Automation tools like Deployment Manager and Terraform become essential.

------------------------------
| Module 4: Managed Services |
------------------------------
In this module, we give you an overview of BigQuery, Cloud Dataflow, Cloud Dataprep by Trifacta, and Cloud Dataproc. Now all of these services are for data analytics purposes, and since that’s not the focus of this course series, there won’t be any labs in this module. Instead, we’ll have a quick demo to illustrate how easy it is to use a managed service.

* Objectives
	Describe the managed services for data processing in GCP

4.1 Overview
------------
Managed services are partial or complete solutions offered as a service. 
They exist on a continuum between PaaS and SaaS depending on how much of the internal methods and controls are exposed.

Using a managed service allows you to outsource a lot of the administrative and maintenance overhead to Google if your application requirements fits within the service offering. 

BigQuery, Cloud Dataflow, Cloud Dataprep by Trifecta and Cloud Dataproc
All of these services are for data analytics purposes. 

4.2 Managed Services
--------------------

4.2.1 Video: BigQuery
-----
Serverless, 
highly scalable, 
cost effective Cloud data warehouse. 
Fully managed
Petabyte scale
SQL Interface
Very fast
Free usage tier

Example:
A query on the table with over 100 billion rows. This query processes over 4.1 terabyte, but takes less than a minute to execute. The same query would take hours if not days through a serial execution.

4.2.2 Video: Cloud Dataflow
-----
Serverless and fully managed service data processing patterns
Batch and stream auto processing
Open source programming using Beam
Intelligently scale to Mns of QPS

4.2.3 Video: Cloud Dataprep
-----
Serverless, works at any scale
Suggests ideal data transmission
Focus on data analysis
Integrated partner servive operated by Trifacta

4.2.4 Video: Cloud Dataproc
4.2.5 Video: Demo: Cloud Dataproc
-----
Fast, easy to use
for running Apache Spark and Hadoop

Low cost: /sec billing and preemptible
Super fast to start, scale and shutdown
Integrated with GCP
Managed Service
Simple and familiar

4.3 Review
-----------

4.3.1 Quiz: Module Quiz
-----
Done - 2 questions

4.3.2 Video: Module Review
-----
Managed services allow you to outsource a lot of the administrative and maintenance overhead to Google, so you can focus on your workloads instead of infrastructure. 

Speaking of Infrastructure, most of the services that we covered our serverless. 

Now, this doesn't mean that there aren't any actual servers processing your data. Serverless means that servers or compute engine instances are obfuscated so that you don't have to worry about the Infrastructure. Cloud Dataproc isn't a serverless service because you were able to view and manage the underlying master and worker instances.


4.3.4 Reading:What’s Next? Get Certified
-----
Associate Cloud Engineer certification
	Set up a cloud solution environment
	Plan and configure a cloud solution
	Deploy and implement a cloud solution
	Ensure successful operation of a cloud solution
	Configure access and security

Check Lab: Cloud Functions: Qwik Start - Console.zip
https://google.qwiklabs.com/focuses/1763?catalog_rank=%7B"rank"%3A1%2C"num_filters"%3A0%2C"has_search"%3Atrue%7D&parent=catalog&search_id=3434733

App Dev - Deploying the Application into App Engine Flexible Environment - Java
https://google.qwiklabs.com/focuses/1060?catalog_rank=%7B"rank"%3A1%2C"num_filters"%3A0%2C"has_search"%3Atrue%7D&parent=catalog&search_id=3434791

App Dev: Deploying the Application into App Engine Flexible Environment - Python
https://google.qwiklabs.com/focuses/1072?catalog_rank=%7B"rank"%3A1%2C"num_filters"%3A0%2C"has_search"%3Atrue%7D&parent=catalog&search_id=3434795